<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book [
<!-- Some useful entities borrowed from HTML -->
        <!ENTITY ndash "&#x2013;">
        <!ENTITY mdash "&#x2014;">
        <!ENTITY hellip "&#x2026;">

        <!-- Useful for describing APIs -->
        <!ENTITY GET '<command xmlns="http://docbook.org/ns/docbook">GET</command>'>
        <!ENTITY PUT '<command xmlns="http://docbook.org/ns/docbook">PUT</command>'>
        <!ENTITY POST '<command xmlns="http://docbook.org/ns/docbook">POST</command>'>
        <!ENTITY DELETE '<command xmlns="http://docbook.org/ns/docbook">DELETE</command>'>
        <!ENTITY PRODNAME "Cloud Big Data">
        <!ENTITY PRODABBV "CBD">
        
        <!-- changing authentication endpoints; define entities for US & UK rather than maintaining in text -->
        <!ENTITY ENDPOINT-US "https://identity.api.rackspacecloud.com/v1.1/">
        <!ENTITY ENDPOINT-UK "https://lon.identity.api.rackspacecloud.com/v1.1/">
        <!ENTITY ENDPOINT-US-20 "https://identity.api.rackspacecloud.com/v2.0/">
        <!ENTITY ENDPOINT-UK-20 "https://lon.identity.api.rackspacecloud.com/v2.0/">

        <!-- Useful for specs -->
        <!ENTITY MAY '<emphasis xmlns="http://docbook.org/ns/docbook" role="strong">MAY</emphasis>'>
        <!ENTITY SHOULD '<emphasis xmlns="http://docbook.org/ns/docbook" role="strong">SHOULD</emphasis>'>
        <!ENTITY MUST '<emphasis xmlns="http://docbook.org/ns/docbook" role="strong">MUST</emphasis>'>
        <!ENTITY MUST_NOT '<emphasis xmlns="http://docbook.org/ns/docbook" role="strong">MUST NOT</emphasis>'>

        <!ENTITY CHECK '<inlinemediaobject xmlns="http://docbook.org/ns/docbook">
                 <imageobject>
                  <imagedata fileref="img/Check_mark_23x20_02.svg"
                             format="SVG" scale="60"/>
                </imageobject>
              </inlinemediaobject>'>

        <!ENTITY ARROW '<inlinemediaobject xmlns="http://docbook.org/ns/docbook">
                 <imageobject>
                  <imagedata fileref="img/Arrow_east.svg"
                             format="SVG" scale="60"/>
                </imageobject>
              </inlinemediaobject>'>
]>

<book version="5.0"
         xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML"
         xmlns:html="http://www.w3.org/1999/xhtml"
         xml:id="cbd-getting-started">
    
    <?rax title.font.size="35px" subtitle.font.size="20px"?>
    <title>Rackspace Cloud Big Data Getting Started Guide</title>
    <?rax status.bar.text.font.size="40px"
          status.bar.text="Limited Access"?>   
    <titleabbrev>Rackspace Cloud Big Data Getting Started
        Guide</titleabbrev>
     
    <info>
        
        <copyright>
            <year>2013</year>
            <year>2014</year>
            <holder>Rackspace US, Inc.</holder>
        </copyright>
        <releaseinfo>API v1.0 Limited Access</releaseinfo>
        <productname>Rackspace Cloud Big Data</productname>
        <pubdate>2014-01-14</pubdate>
        <legalnotice role="rs-api">
            <annotation>
                <remark>Copyright details are filled in by the doc build process.</remark>
            </annotation>
        </legalnotice>
        <abstract>
            <para>This document is intended for software developers
                interested in developing service management
                applications using the Rackspace &PRODNAME;
                Application Programming Interface
                    (<abbrev>API</abbrev>). </para>
        </abstract>
        <revhistory>
            <revision>
                <date>2014-01-14</date>
                <revdescription>
                    <itemizedlist spacing="compact">
                        <listitem>
                            <para>Initial Limited Access (LA)
                                release.</para>
                        </listitem>
                    </itemizedlist>
                </revdescription>
            </revision>
            <revision>
                <date>2013-11-01</date>
                <revdescription>
                    <itemizedlist>
                        <listitem>
                            <para>Added <xref
                                   linkend="List-Cluster-Types-d1e002"
                                /> to <xref
                                   linkend="Create_Manage_Clusters-d1e001"
                                />.</para>
                        </listitem>
                        <listitem>
                            <para>Added <xref
                                   linkend="List-Clusters-d1e004"/> to
                                   <xref
                                   linkend="Create_Manage_Clusters-d1e001"
                                />.</para>
                        </listitem>
                    </itemizedlist>
                </revdescription>
            </revision>
            <revision>
                <date>2013-09-16</date>
                <revdescription>
                    <itemizedlist spacing="compact">
                        <listitem>
                            <para>Initial Early Access (EA)
                                release.</para>
                        </listitem>
                    </itemizedlist>
                </revdescription>
            </revision>
        </revhistory>
    <raxm:metadata xmlns:raxm="http://docs.rackspace.com/api/metadata">
      <raxm:displayname>Getting Started Guide</raxm:displayname>
      <raxm:product version="v1.0">cbd</raxm:product>
      <raxm:priority>10</raxm:priority>
    </raxm:metadata>
    </info>
    <chapter xml:id="DB_Doc_Change_History">
        <title>Document Change History</title>            
        <para>This version of the guide replaces and obsoletes all
            earlier versions. The most recent changes are described in
            the following table:</para>
        <?rax revhistory?>
    </chapter>
      <chapter xml:id="CBD_Overview">
        <title>Overview</title>
        <para xmlns:wadl="http://wadl.dev.java.net/2009/02">Rackspace
            &PRODNAME; is an on-demand Apache Hadoop service for the
            Rackspace open cloud. The service supports a RESTful API
            and alleviates the pain associated with deploying,
            managing, and scaling Hadoop clusters. </para>
        <para xmlns:wadl="http://wadl.dev.java.net/2009/02">Cloud Big
            Data is just as flexible and feature-rich as Hadoop. With
            Cloud Big Data, you benefit from on-demand servers,
            utility-based pricing, and access to the full set of
            Hadoop features and APIs. However, you do not have to
            worry about provisioning, growing, or maintaining your
            Hadoop infrastructure. The Cloud Big Data service uses an
            environment that is specifically optimized for Hadoop,
            which ensures that your jobs run efficiently and reliably.
            Note that you are still responsible for developing,
            troubleshooting, and deploying your applications. </para>
        <remark xmlns:wadl="http://wadl.dev.java.net/2009/02"
            >Reviewers: If you have an architecture diagram for Cloud
            Big Data that you want included here, please send it to
            me.</remark>
<!-- Coming after EA release - David Dobbins is working with marketing to create this.          
        <para 
            xmlns:wadl="http://wadl.dev.java.net/2009/02">The
            following figure shows an overview of Cloud Big Data: </para>
          <para>&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;Figure
            Coming by 9/12/2013>>>>>>>>>></para>
-->            
        <para xmlns:wadl="http://wadl.dev.java.net/2009/02">The
            primary use cases for &PRODNAME; are as follows: </para>
        <itemizedlist xmlns:wadl="http://wadl.dev.java.net/2009/02">
            <listitem>
                <para>Create on-demand infrastructure for applications
                    in production where physical servers would be too
                    costly and time-consuming to configure and
                    maintain.</para>
            </listitem>
            <listitem>
                <para>Develop, test, and pilot data analysis
                    applications.</para>
            </listitem>
        </itemizedlist>
        <para xmlns:wadl="http://wadl.dev.java.net/2009/02">&PRODNAME;
            provides the following benefits: <itemizedlist>
                <listitem>
                    <para>Create or resize Hadoop clusters in minutes
                        and pay only for what you use.</para>
                </listitem>
                <listitem>
                    <para>Access the Hortonworks Data Platform (HDP),
                        an enterprise-ready distribution that is 100
                        percent Apache open source.</para>
                </listitem>
                <listitem>
                    <para>Provision and manage Hadoop through an
                        easy-to-use Control Panel and a RESTful
                        API.</para>
                </listitem>
                <listitem>
                    <para>Seamlessly access data in Cloud Files
                        containers.</para>
                </listitem>
                <listitem>
                    <para>Gain interoperability with any third-party
                        software tool that supports HDP.</para>
                </listitem>
                <listitem>
                    <para>Access Fanatical SupportÂ® on a 24x7x365
                        basis via chat, phone, or ticket.</para>
                </listitem>
            </itemizedlist>
        </para>
        <para>This guide provides the following ways to use the Cloud
            Big Data API:</para>
        <itemizedlist>
            <listitem>
                <para>Using the API directly (<xref
                        linkend="Using_the_API_Directly"/>)</para>
            </listitem>
            <listitem>
                <para>Using a Python Lava command-line client  (<xref
                        linkend="Using_Python_lavaclient"/>)</para>
            </listitem>
        </itemizedlist>
        <para>Follow the steps described in this guide to start using
            the Rackspace &PRODNAME; API.</para>
        <section xml:id="core_concepts">
            <title>Limited Access Program</title>
            <para xmlns:wadl="http://wadl.dev.java.net/2009/02">The
                Limited Access program enables customers and Rackers
                to test an early version of the product and provide
                feedback on the product and its capabilities.
                Customers are screened as part of the on-boarding
                process to ensure correct product/customer fit. </para>
            <para xmlns:wadl="http://wadl.dev.java.net/2009/02"
                >Limited Access users must meet the following
                criteria: <itemizedlist spacing="compact">
                    <listitem>
                        <para>A Rackspace Cloud account</para>
                    </listitem>
                    <listitem>
                        <para>Prior knowledge of Hadoop or a
                            third-party tool that works with
                            Hadoop</para>
                    </listitem>
                    <listitem>
                        <para>Ability to work with the Hortonworks
                            Data Platform (HDP)</para>
                    </listitem>
                    <listitem>
                        <para>Prior knowledge of HTTP/1.1
                            conventions</para>
                    </listitem>
                    <listitem>
                        <para>Basic familiarity with Cloud and RESTful
                            APIs</para>
                    </listitem>
                    <listitem>
                        <para>A willingness to provide feedback to the
                            Rackspace Product and Product Marketing
                            functions</para>
                    </listitem>
                </itemizedlist>
            </para>
            <para>By using the &PRODNAME; API, you understand and
                agree to the following limitations and
                conditions:</para>
            <itemizedlist>
                <listitem>
                    <para>If you participate in this Limited Access
                        program, you will receive a notification prior
                        to general availability.</para>
                </listitem>
                <listitem>
                    <para>The API contract is not locked and might
                        change during the Limited Access program. If
                        the contract changes, Rackspace will notify
                        customers in release notes. </para>
                </listitem>
            </itemizedlist>
        </section>
      <section xml:id="Pricing_SLA-d1e1362">
            <title>Pricing and Service Level</title>
            <para>&PRODNAME; is part of the Rackspace Cloud and your
                use through the API will be billed as per the pricing
                schedule at <link
                    xlink:href="http://www.rackspace.com/cloud/public/bigdata/pricing"
                    >http://www.rackspace.com/cloud/public/bigdata/pricing</link>.
                Cloud Servers is also part of the Rackspace Cloud and
                your use through the Cloud Control Panel will be
                billed as per the pricing schedule at <link
                    xlink:href="http://www.rackspace.com/cloud/public/servers/pricing"
                    >http://www.rackspace.com/cloud/public/servers/pricing</link>.</para>
         
            <para>The Service Level Agreements (SLAs) for Cloud
                Big Data and Cloud Servers are available at <link
                    xlink:href="http://www.rackspace.com/cloud/legal/sla"
                    >http://www.rackspace.com/cloud/legal/sla</link>.</para>
               
        </section> 
        <section xml:id="Prerequisites_Examples">
            <title>Prerequisites for Running Examples</title>
            <para>To run the examples in this guide, you must have the
                following prerequisites: <itemizedlist
                    spacing="compact">
                    <listitem>
                        <para>Rackspace Cloud account</para>
                    </listitem>
                    <listitem>
                        <para>Rackspace Cloud user name and password,
                            as specified during registration</para>
                    </listitem>
                </itemizedlist></para>
        </section>
    </chapter>
    <chapter xml:id="Service_Access_Endpoints-d1e753">
        <title>Service Access Endpoints</title>
        <para xmlns:wadl="http://wadl.dev.java.net/2009/02">The
            &PRODNAME; service is a regionalized service. The user of
            the service is therefore responsible for appropriate
            replication, caching, and overall maintenance of
            &PRODNAME; data across regional boundaries to other Cloud
            Servers.</para>
        <para xmlns:wadl="http://wadl.dev.java.net/2009/02">The
            endpoints to use for your Cloud Big Data API calls are
            summarized in the table below.</para>
        <para xmlns:wadl="http://wadl.dev.java.net/2009/02">
            <table rules="all">
                <caption>Regionalized Service Endpoints</caption>
                <thead>
                    <tr align="center">
                        <td colspan="2">Region</td>
                        <td colspan="5">Endpoint</td>
                    </tr>
                </thead>
                <tbody>
                    <tr align="left">
                        <td colspan="2">Chicago (ORD)</td>
                        <td colspan="5"
                                ><code>https://ord.bigdata.api.rackspacecloud.com/v1.0/</code><parameter><replaceable>yourAccountID</replaceable></parameter>/
                        </td>
                    </tr>
                    <tr align="left">
                        <td colspan="2">Dallas/Ft. Worth (DFW)</td>
                        <td colspan="5"
                                ><code>https://dfw.bigdata.api.rackspacecloud.com/v1.0/</code><parameter><replaceable>yourAccountID</replaceable></parameter>/
                        </td>
                    </tr>
                    <tr align="left">
                        <td colspan="2">London (LON)</td>
                        <td colspan="5"
                                ><code>https://lon.bigdata.api.rackspacecloud.com/v1.0/</code><parameter><replaceable>yourAccountID</replaceable></parameter>/
                        </td>
                    </tr>
                </tbody>
            </table>
        </para>
        <para xmlns:wadl="http://wadl.dev.java.net/2009/02">Replace
            the <code><replaceable>yourAccountID</replaceable></code>
            placeholder with your actual account number, , which is
            returned as part of the authentication service response,
            after the final '/' in the <code>publicURL</code> field. </para>
        <note>
            <para>All examples in this guide assume that you are
                operating against the DFW data center. If you are
                using a different data center, be sure to use the
                associated endpoint from the table above.</para>
        </note>
        <para>When you perform a Cloud Big Data API operation, place
            the endpoint at the beginning of the request URL. For
            example:
                <code>https://dfw.bigdata.api.rackspacecloud.com/v1.0/yourAccountID/</code>.</para>
    </chapter>
    <chapter xml:id="Using_the_API_Directly">
        <title>Using the API Directly</title>
        <para>This chapter describes how to use Cloud Big Data by
            using the API directly.</para>
      
    <section xml:id="DB_Sending_API_Requests">
        <title>Sending Requests to the API</title>
        <para>You have several options for sending requests through an
            API: </para>
        <itemizedlist>
            <listitem>
                <para>Developers and testers might prefer to use cURL,
                        the command-line tool from <link
                            xlink:href="http://curl.haxx.se/"
                            >http://curl.haxx.se/</link>. With cURL,
                        you can send HTTP requests and receive
                        responses back from the command line. </para>
            </listitem>
            <listitem>
                <para>If you like to use a more graphical interface,
                        the REST client for Mozilla Firefox also works
                        well for testing and trying out commands. See
                            <link
                            xlink:href="https://addons.mozilla.org/en-US/firefox/addon/restclient/"
                            >https://addons.mozilla.org/en-US/firefox/addon/restclient/</link>. </para>
            </listitem>
            <listitem>
                <para>You can download and install RESTClient, a Java
                        application used to test RESTful web services,
                        from <link
                            xlink:href="http://code.google.com/p/rest-client/"
                            >http://code.google.com/p/rest-client/</link>. </para>
            </listitem>
        </itemizedlist>
        <section xml:id="DB_using-curl-cli">
            <title>Sending API Requests by Using cURL</title>
            <para>cURL is a command-line tool that is available in
                    most UNIX system-based environments and Apple Mac
                    OS X systems, and can be downloaded for Microsoft
                    Windows to interact with REST interfaces. For more
                    information about cURL, visit <link
                        xlink:href="http://curl.haxx.se/"
                        >http://curl.haxx.se/</link>. </para>
            <para>cURL enables you to transmit and receive HTTP
                    requests and responses from the command line or
                    from within a shell script. As a result, you can
                    work with the REST API directly without using one
                    of the client APIs.</para>
            <para>The following cURL command-line options are used in
                    this guide to run the examples:</para>
                <table rules="all">
                    <caption>cURL Command-Line Options</caption>
                    <col width="25%"/>
                    <col width="75%"/>
                    <thead>
                        <tr>
                            <th>Option</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><option>-d</option></td>
                            <td>Sends the specified data in a POST
                                request to the HTTP server</td>
                        </tr>
                        <tr>
                            <td><option>-i</option></td>
                            <td>Includes the HTTP header in the
                                output</td>
                        </tr>
                        <tr>
                            <td><option>-H HEADER</option></td>
                            <td>Specifies an HTTP header in the
                                request</td>
                        </tr>
                        <tr>
                            <td><option>-X</option></td>
                            <td>Specifies the request operation to use
                                when communicating with the HTTP
                                server. The specified request is used
                                instead of the default operation,
                                which is GET. For example, <option>-X
                                   PUT</option> specifies to use the
                                PUT request operation.</td>
                        </tr>
                    </tbody>
                </table>
            <note>
                <para>If you have the tools, you can run the cURL JSON
                        request examples with the following options to
                        format the output from cURL:
                            <?sbr?><command>&lt;curl JSON request
                            example> | python
                        -mjson.tool</command>.</para>
            </note>
                <procedure>
                    <title>To run the cURL request examples</title>
                    <para>To run the cURL request examples shown in
                        this guide on Linux or Mac systems, perform
                        the following actions:</para>
                    <step>
                        <para>Copy each example from the HTML version
                            of this guide an paste it into an ASCII
                            text editor (for example, vi or TextEdit).
                        </para>
                    </step>
                    <step>
                        <para>Modify each example with your required
                            account information and other information,
                            as detailed in this guide.</para>
                        <note>
                            <para>The carriage returns in the cURL
                                request examples that are part of the
                                cURL syntax are escaped with a
                                backslash (\) to avoid prematurely
                                terminating the command. However, you
                                should not escape carriage returns
                                inside the XML or JSON message within
                                the command.</para>
                        </note>
                        <para>Consider the following cURL
                            Authentication Request: XML example, which
                            is described in detail in <xref
                                linkend="Generating_Auth_Token"
                            />.</para>
                        <example>
                            <title>cURL Authenticate Request:
                                XML</title>
                            <?dbfo keep-together="always"?>
                            <screen language="bash"><command>curl</command> <option>-i</option> <option>-d</option> \
'&lt;?xml version="1.0" encoding="UTF-8"?>
 &lt;auth&gt;   
    &lt;apiKeyCredentials     
        xmlns="http://docs.rackspace.com/identity/api/ext/RAX-KSKEY/v1.0"     
        username="<emphasis role="bold">yourUserName</emphasis>"     
        apiKey="<emphasis role="bold">yourApiKey</emphasis>"/&gt;   
 &lt;/auth&gt;<option>'</option> \
<uri>'https://identity.api.rackspacecloud.com/v2.0/tokens'</uri></screen>
                        </example>
                        <para>The lines that are part of the cURL
                            command syntax have been escaped with a
                            backslash (\) to indicate that the command
                            continues on the next line:</para>
                        <screen language="bash"><command>curl</command> <option>-i</option> <option>-d</option> \
  
   
(... lines within the XML portion of the message are not shown in this example)
(... the example shows only lines that are part of cURL syntax)     
     

   
 &lt;/auth&gt;<option>'</option> \
<uri>'https://identity.api.rackspacecloud.com/v2.0/tokens'</uri></screen>
                        <para>However, the lines <emphasis
                                role="italic">within</emphasis> the
                            XML portion of the message are not escaped
                            with a backslash, to avoid issues with XML
                            processing. </para>
                        <screen language="bash">'&lt;?xml version="1.0" encoding="UTF-8"?>
 &lt;auth&gt;   
    &lt;apiKeyCredentials     
        xmlns="http://docs.rackspace.com/identity/api/ext/RAX-KSKEY/v1.0"     
        username="<emphasis role="bold">yourUserName</emphasis>"     
        apiKey="<emphasis role="bold">yourApiKey</emphasis>"/&gt;   
 &lt;/auth&gt;<option>'</option> \
</screen>
                        <para>The final line of the XML message is
                            escaped, but the backslash lies outside
                            the XML message and continues the cURL
                            command to the next line. </para>
                    </step>
                    <step>
                        <para>After you are finished modifying the
                            text for the cURL request example with
                            your information (for example, user name
                            and API key), copy and paste the text into
                            your terminal window. </para>
                        <tip>
                            <para>If you have trouble copying and
                                pasting the examples as described, try
                                typing the entire example on one long
                                line, removing all the backslash
                                line-continuation characters.</para>
                        </tip>
                    </step>
                    <step>
                        <para>Press <keycombo>
                                <keycap>Enter</keycap>
                            </keycombo> to run the cURL
                            command.</para>
                    </step>
                </procedure>
        </section>
    </section>
    <section xml:id="Generating_Auth_Token">
        <title>Generating an Authentication Token</title>
            <para>Whether you use cURL or a REST client to interact
                with the Cloud Big Data API, you must generate an
                authentication token. You provide this token in the
                X-Auth-Token header in each Cloud Big Data API
                request. <xref linkend="XML-Auth-Request-d1003"/> and
                    <xref linkend="JSON-Auth-Request-d1003"/>
                demonstrate how to use cURL to obtain the
                authentication token as well as your account number.
                You must provide both when making subsequent Cloud Big
                Data API requests.</para>
        <para>Remember to replace the placeholders in the following
                authentication request examples with your
                    information:<itemizedlist spacing="compact">
                    <listitem>
                        <para><emphasis role="bold"
                                >yourUserName</emphasis> &mdash;Your
                            common Rackspace Cloud user name, as
                            supplied during registration.</para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold"
                                >yourApiKey</emphasis> &mdash; Your
                            API access key. You can obtain the key
                            from the <link
                                xlink:href="http://mycloud.rackspace.com/"
                                >Rackspace Cloud Control Panel</link>
                            in the <guimenu>Your Account</guimenu> /
                                <guimenuitem>API Keys</guimenuitem>
                            section.</para>
                    </listitem>
                </itemizedlist></para>
        <para> You can use either of the following endpoints to access
                the Cloud Authentication service: </para>
        <itemizedlist spacing="compact">
            <listitem>
                <para><link xlink:href="&ENDPOINT-US-20;"
                        >&ENDPOINT-US-20;</link>
                </para>
            </listitem>
            <listitem>
                <para><link xlink:href="&ENDPOINT-UK-20;"
                        >&ENDPOINT-UK-20;</link></para>
            </listitem>
        </itemizedlist>
        <para> Your account might be based in either the US or the UK.
                The location of your account is not determined by your
                physical location but by the location of the Rackspace
                retail site that was used to create your account: </para>
        <itemizedlist spacing="compact">
            <listitem>
                <para> If your account was created via <link
                        xlink:href="http://www.rackspacecloud.com"
                        >http://www.rackspacecloud.com</link>, it is a
                    US-based account. </para>
            </listitem>
            <listitem>
                <para> If your account was created via <link
                        xlink:href="http://www.rackspace.co.uk"
                        >http://www.rackspace.co.uk</link>, it is a
                    UK-based account. </para>
            </listitem>
        </itemizedlist>
        <para> If you are unsure how your account was created, use the
            Rackspace contact information at either site to ask for
            help. </para>  
        <para>Notice that you authenticate by using a special URL for
                the Cloud Authentication services. For example, for
                US-based accounts, you use
                    <code>https://identity.api.rackspacecloud.com/v2.0/tokens</code>,
                as shown in the following Authenticate Request
                examples. Note that the <code>v2.0</code> component in
                the URL indicates that you are using version 2.0 of
                the Cloud Authentication API.</para>
            <note>
                <para>For UK-based accounts, you can use
                        <code>'https://lon.identity.api.rackspacecloud.com/v2.0/tokens'</code>
                    instead.</para>
            </note>
        <example xml:id="XML-Auth-Request-d1003">
            <title>cURL Authenticate Request: XML</title>
            <?dbfo keep-together="always"?>
            <screen language="bash"><command>curl</command> <option>-i</option> <option>-d</option> \
'&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;auth>
   &lt;apiKeyCredentials
      xmlns="http://docs.rackspace.com/identity/api/ext/RAX-KSKEY/v1.0"
         username="<emphasis role="bold">yourUserName</emphasis>"
         apiKey="<emphasis role="bold">yourApiKey</emphasis>"/>
&lt;/auth>' \
<option>-H 'Content-Type: application/xml'</option> \
<option>-H 'Accept: application/xml'</option> \
<uri>'https://identity.api.rackspacecloud.com/v2.0/tokens'</uri></screen>
        </example>
        <example xml:id="JSON-Auth-Request-d1003">
            <title>cURL Authenticate Request: JSON</title>
            <?dbfo keep-together="always"?>
            <screen language="bash"><command>curl</command> <option>-i</option> <option>-d</option> \
'{
    "auth":
    {
       "RAX-KSKEY:apiKeyCredentials":
       {
          "username": "<emphasis role="bold">yourUserName</emphasis>",
          "apiKey": "<emphasis role="bold">yourApiKey</emphasis>"}
    }
}' \
<option>-H 'Content-Type: application/json'</option> \
<uri>'https://identity.api.rackspacecloud.com/v2.0/tokens'</uri></screen>
        </example>
            <para>In the authentication response (examples follow),
                the authentication <code>token id</code> is returned
                with an <code>expires</code> attribute that specifies
                when the token expires. Remember to supply your
                authentication token wherever you see the placeholder
                    <emphasis role="bold">yourAuthToken</emphasis> in
                the examples in this guide.</para>
            <note>
                <title>Notes</title>
                <itemizedlist spacing="compact">
                    <listitem>
                        <para>The values that you receive in your
                            responses vary from the examples shown in
                            this document because they are specific to
                            your account.</para>
                    </listitem>
                    <listitem>
                        <para>The information shown in the
                            authentication response examples is for
                            US-based accounts. If you authenticate
                            against the UK-endpoint for the
                            Authentication service, you see the
                            service catalog information for UK-based
                            accounts.</para>
                    </listitem>
                    <listitem>
                        <para> The <code>expires</code> attribute
                            denotes the time after which the token
                            will automatically become invalid. A token
                            might be manually revoked before the time
                            identified by the <code>expires</code>
                            attribute. The attribute predicts a
                            token's maximum possible lifespan but does
                            not guarantee that it will reach that
                            lifespan. Clients are encouraged to cache
                            a token until it expires.</para>
                    </listitem>
                    <listitem>
                        <para>Applications should be designed to
                            re-authenticate after receiving a 401
                            (Unauthorized) response from a service
                            endpoint.</para>
                    </listitem>
                </itemizedlist>
            </note>
            <para>The <code>publicURL</code> endpoints for &PRODNAME;
                (for example
                    <code>https://dfw.bigdata.api.rackspacecloud.com/v1.0/1100111</code>)
                are also returned in the response. </para>
            <para>Your actual account number is after the final slash
                (/) in the <code>publicURL</code> field. In the
                following examples, the account number is 1100111. You
                must specify <emphasis role="italic">your</emphasis>
                account number on most of the &PRODNAME; API
                operations, wherever you see the placeholder <emphasis
                    role="bold">yourAccountID</emphasis> specified in
                the examples in this guide.</para>
            <para>After authentication, you can use cURL to perform
                GET, DELETE, and POST requests for the &PRODNAME;
                API.</para>
        <example>
            <title>Authenticate Response: XML</title>
            <programlisting language="xml"><xi:include href="../../src/resources/samples/auth-20.xml" parse="text"/></programlisting>
        </example>

        <example>
            <title>Authenticate Response: JSON</title>
            <programlisting language="json"><xi:include href="../../src/resources/samples/auth-20.json" parse="text"/></programlisting>
        </example>
    </section>


    <section xml:id="profileSetupView">
        <title>Setting Up and Viewing a Profile</title>
        <remark>Reviewer: Is there an API call to set up a Profile?
                I'm working from the Walk-thrus on the wiki and it
                includes this. But I see no corresponding API call in
                the current API specifications. 9/6/2013 Received
                example POST  request and response from David Dobbins
                for setting up a profile. Added it here on 9/9/2013. </remark>
        <para>Before you can create Hadoop clusters, you must create a
                profile, which consists of a user name and password. </para>
            <note>
                <para>Your Cloud Big Data profile is different from
                    your cloud account. Your profile has the following
                    characteristics and requirements:</para>
                <itemizedlist>
                    <listitem>
                        <para>A profile is the configuration for the
                            administration and login account for the
                            cluster.</para>
                    </listitem>
                    <listitem>
                        <para>Only one profile is allowed for each
                            user or account.</para>
                    </listitem>
                    <listitem>
                        <para>Any updates or additions override the
                            existing profile. </para>
                    </listitem>
                </itemizedlist>
            </note>
        <para>After you create a profile, every Hadoop server that you
                provision by using the API will have a local user
                account with the specified user name and password.
                This allows you to remotely SSH into a server to
                transfer data, run or troubleshoot jobs, and so
                on.</para>
        <para>A password must be at least eight characters, with a
                mixture of uppercase, lowercase, numeric, and special
                characters.</para>
        <para>The following example shows the cURL request to set up a
                profile.</para>
        <remark>Reviewer: If there is a Create Profile operation,
            please provide appropriate cURL request and response. </remark>
        <example>
            <title>cURL  Create Profile Request: JSON</title>
            <programlisting language="bash">curl -i -X POST https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/profile \
-H "Accept: application/json" \
-H "X-Auth-Token: <emphasis role="bold">yourAuthToken</emphasis>" \
-H "Content-Type: application/json" -d\
'{
    "profile": {
        "username": "myuser",
        "password": "T0pS3cret!",
        "sshkeys": [
            {
                "publicKey": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDV9lu1CPeRmnTrDigLOU2lM60igHW4LXEgei9ajMcuUnJXy
UoTthHRlXpWG+D3H1Az/AT1yPqYEubwBWZOJN3r3hvoqYaJYEKAE36vHceEJ89yLuu2PIMheRyhBgmapHCz9d3IGezuuKh/DJRVl8Q7pICcsRIhQxHijcyfcPsPnTfDQ89LKg6k4uyxZZobQozXH/vwtAcsGAW6hX6hsxlvBIqndPPMw2FXkGt56W+YuNNNQC+P+qTyBQ8mWhy1OQJwvn9N/d0wdUAqogLfTasUXeTRjr+Q7ASC/QZ6ewqgrBzLbbM7SqobhNcD6O2suUGr2uvGzKY4a3oaGO68ur2Z testuser@rackspace.com",
                "name": "mykey"
            }
        ],
        "cloudCredentials": {
            "username": "myuser",
            "apikey": "e96406df8e9a82565deb57f3555054b8"
        }
    }
}' </programlisting>
        </example>
            <para>Remember to replace the placeholders in the example
                with your actual values for all the cURL examples in
                this guide:</para>
            <itemizedlist>
                <listitem>
                    <para><emphasis role="bold"
                            >yourAuthToken</emphasis> &mdash; Your
                        authentication token, which is returned in
                        your authentication response (see the response
                        examples in <xref
                            linkend="Generating_Auth_Token"/>)</para>
                </listitem>
                <listitem>
                    <para><emphasis role="bold"
                            >yourAccountID</emphasis> &mdash; Your
                        account ID, which is returned in your
                        authentication response (must be replaced in
                        the request URL)</para>
                </listitem>
            </itemizedlist>
        <para>The following example shows the response for the profile
                request.</para>
        <example>
            <title> Create Profile Response: JSON</title>
            <programlisting language="json">{
    "profile": {
        "username": "myuser",
        "links": [
            {
                "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/681390/profile",
                "rel": "self"
            },
            {
                "href": "https://dfw.bigdata.api.rackspacecloud.com/681390/profile",
                "rel": "bookmark"
            }
        ],
        "cloudCredentials": {
            "username": "myuser"
        },
        "userId": "194683",
        "sshkeys": [
            {
                "publicKey": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDV9lu1CPeRmnTrDigLOU2lM60igHW4LXEgei9ajMcuUnJXy
UoTthHRlXpWG+D3H1Az/AT1yPqYEubwBWZOJN3r3hvoqYaJYEKAE36vHceEJ89yLuu2PIMheRyhBgmapHCz9d3IGezuuKh/DJRVl8Q7pICcsRIhQxHijcyfcPsPnTfDQ89LKg6k4uyxZZobQozXH/vwtAcsGAW6hX6hsxlvBIqndPPMw2FXkGt56W+YuNNNQC+P+qTyBQ8mWhy1OQJwvn9N/d0wdUAqogLfTasUXeTRjr+Q7ASC/QZ6ewqgrBzLbbM7SqobhNcD6O2suUGr2uvGzKY4a3oaGO68ur2Z testuser@rackspace.com",
                "name": "mykey"
            }
        ],
        "tenantId": "681390"
    }
}</programlisting>
        </example>
        <para>Anytime after you create your profile, you can view its
                details by using the Get Profile operation. The
                response displays your profile details, including your
                user name, user ID, and tenant ID. </para>
        <informaltable rules="all"
            xmlns:wadl="http://wadl.dev.java.net/2009/02">
            <thead>
                <tr align="center">
                    <td colspan="1">Verb</td>
                    <td colspan="2">URI</td>
                    <td colspan="3">Description</td>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td colspan="1">&GET;</td>
                    <td colspan="2">/profile</td>
                    <td colspan="3">Returns detailed information for
                        the current user profile.</td>
                </tr>
            </tbody>
        </informaltable>
        <para>This operation does not require a request body.</para>
        <para>The following examples show the cURL request and
                corresponding response for viewing a profile.</para>
        <remark>Reviewer: Please provide appropriate cURL to produce
            the response given in this example.</remark>
        <example>
            <title>cURL  Get Profile Request: JSON</title>
            <programlisting language="bash">curl -i -X GET https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/profile -d  \
-H "X-Auth-Token: <emphasis role="bold">yourAuthToken</emphasis>" \
-H "Accept: application/json"\
-H "Content-type: application/json"</programlisting>
        </example>
        <example>
            <title> Get Profile Response: JSON</title>
            <programlisting language="json"> {"profile":
            { "username": "john.doe",
              "user_id" : "12346",
              "tenant_id" : "123456",
              "sshkeys": [{"name": "t@test"}],
              "cloudCredentials": {},
              "links": [
                { "rel": "self",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/123456/profile"
                },
                { "rel": "bookmark",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/123456/profile"
                }
              ],
            }
        }</programlisting>
        </example>
    </section>
    
    <section xml:id="Create_DB_Instance">
            <title>Viewing Resource Limits</title>
            <para>The use of the Rackspace Cloud Big Data API is
                subject to resource limits. You can view the limits
                associated with your account by using the View
                Resource Limits operation. This operation displays
                resource limits such as remaining node count,
                available RAM, and remaining disk space, for the
                user.</para>
        <informaltable rules="all"
            xmlns:wadl="http://wadl.dev.java.net/2009/02">
            <thead>
                <tr align="center">
                    <td colspan="1">Verb</td>
                    <td colspan="2">URI</td>
                    <td colspan="3">Description</td>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td colspan="1">&GET;</td>
                    <td colspan="2">/limits</td>
                    <td colspan="3">Displays the resource limits for
                        the user.</td>
                </tr>
            </tbody>
        </informaltable>
        <para>This operation does not require a request body.</para>
        <para>The following show the cURL request and corresponding
                response for viewing resource limits.</para>
        <remark>Reviewer: Please provide appropriate cURL to produce
            the response given in this example.</remark>
        <example>
            <title>cURL View Resource Limits Request: JSON</title>
            <programlisting language="bash">curl -i -X GET https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/limits -d \
-H "X-Auth-Token: <emphasis role="bold">yourAuthToken</emphasis>" \
-H "Accept: application/json" \
-H "Content-type: application/json" </programlisting>
        </example>
        <example xmlns:wadl="http://wadl.dev.java.net/2009/02">
            <title>View Resource Limits Response: JSON</title>
<!--  cyr's          <programlisting>        { "limits":
          { "absolute":
            {
              "disk": {
                "limit": 2000,
                "remaining": 2000
              },
              "nodeCount": {
                "limit": 5,
                "remaining": 5
              },
              "ram": {
                "limit": 25600,
                "remaining": 25600
              },
              "vcpus": {
                "limit": 5,
                "remaining": 5
              }
            },
            "links": [
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/limits",
                "rel": "self"
              },
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/1234/limits",
                "rel": "bookmark"
              }
            ]
          }
        }</programlisting> -->
                <programlisting language="json">{
    "limits": {
        "links": [
            {
                "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/681390/limits",
                "rel": "self"
            },
            {
                "href": "https://dfw.bigdata.api.rackspacecloud.com/681390/limits",
                "rel": "bookmark"
            }
        ],
        "absolute": {
            "nodeCount": {
                "limit": 5,
                "remaining": 5
            },
            "vcpus": {
                "limit": 10,
                "remaining": 10
            },
            "ram": {
                "limit": 40960,
                "remaining": 40960
            },
            "disk": {
                "limit": 2048,
                "remaining": 2048
            }
        }
    }
}</programlisting>
        </example>
        </section>
    <section xml:id="Create_Manage_Clusters-d1e001">
        <title>Creating and Managing Hadoop Clusters</title>
        <para>Now you are ready to create and manage Hadoop clusters
                by using the Rackspace Cloud Big Data API. </para>
            <section xml:id="Listing-Flavors-d1003">
                <title>Listing Flavors</title>
                <para>A flavor is an available hardware configuration
                    for a cluster. Each flavor has a unique
                    combination of memory capacity and priority for
                    CPU time. The larger the flavor size you use, the
                    larger the amount of RAM and the higher the
                    priority for CPU time your cluster
                    receives.</para>
                <para>You use the List Flavors operation to find the
                    available configurations for your cluster, and
                    then you decide which size you need for your
                    cluster. You perform this operation to get a list
                    of flavors from which to choose when you create a
                    cluster. </para>
                <informaltable rules="all">
                    <thead>
                        <tr align="center">
                            <td colspan="1">Verb</td>
                            <td colspan="2">URI</td>
                            <td colspan="3">Description</td>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td colspan="1">&GET;</td>
                            <td colspan="2">/flavors</td>
                            <td colspan="3">Lists all available
                                flavors.</td>
                        </tr>
                    </tbody>
                </informaltable>
                <para>This operation does not require a request
                    body.</para>
                <para>The following examples show the cURL request and
                    the corresponding response for the List Flavors
                    operation. </para>
                <remark>Reviewer: Please provide appropriate cURL to
                    produce the response given in this
                    example.</remark>
                <example>
                    <title>cURL List Flavors Request: JSON</title>
                    <programlisting language="bash" >curl -i -X GET https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/flavors -d \
-H "X-Auth-Token: <emphasis role="bold">yourAuthToken</emphasis>" \
-H "Accept: application/json" \
-H "Content-type: application/json"</programlisting>
                </example>
                <example>
                    <title>List Flavors Response: JSON</title>
                    <programlisting language="json">        { "flavors": [
          {
            "disk": 256,
            "id": "4fba3bca-7c76-11e2-b737-beeffa00040e",
            "links": [
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/flavors/4fba3bca-7c76-11e2-b737-beeffa00040e", 
                "rel": "self"
              },
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/1234/flavors/4fba3bca-7c76-11e2-b737-beeffa00040e", 
                "rel": "bookmark"
              }
            ],
            "name": "Tiny Hadoop Instance",
            "ram": 5120,
            "vcpus": 1
          },
          {
            "disk": 512,
            "id": "518400b2-7c76-11e2-b737-beeffa00040e",
            "links": [
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/flavors/518400b2-7c76-11e2-b737-beeffa00040e", 
                "rel": "self"
              },
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/1234/flavors/518400b2-7c76-11e2-b737-beeffa00040e", 
                "rel": "bookmark"
              }
            ],
            "name": "Small Hadoop Instance",
            "ram": 8192,
            "vcpus": 2
          },
          {
            "disk": 1024,
            "id": "5413894c-7c76-11e2-b737-beeffa00040e",
            "links": [
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/flavors/5413894c-7c76-11e2-b737-beeffa00040e", 
                "rel": "self"
              },
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/1234/flavors/5413894c-7c76-11e2-b737-beeffa00040e", 
                "rel": "bookmark"
              }
            ],
            "name": "Medium Hadoop Instance",
            "ram": 15360,
            "vcpus": 4
          },
          {
            "disk": 2000,
            "id": "559dd2b8-7c76-11e2-b737-beeffa00040e",
            "links": [
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/flavors/559dd2b8-7c76-11e2-b737-beeffa00040e", 
                "rel": "self"
              },
              { "href": "http://dfw.bigdata.api.rackspacecloud.com/1234/flavors/559dd2b8-7c76-11e2-b737-beeffa00040e", 
                "rel": "bookmark"
              }
            ],
            "name": "Large Hadoop Instance",
            "ram": 27648,
            "vcpus": 8
          }
        ] }</programlisting>
                </example>
            </section>
            <section xml:id="List-Cluster-Types-d1e002">
                <title>Listing Cluster Types</title>
                <para>You use the List Cluster Types operation to find
                    the cluster types that are available. You perform
                    this operation to get a cluster type to use when
                    you create a cluster.</para>
                <informaltable rules="all">
                    <thead>
                        <tr align="center">
                            <td colspan="1">Verb</td>
                            <td colspan="2">URI</td>
                            <td colspan="3">Description</td>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td colspan="1">&GET;</td>
                            <td colspan="2">/types</td>
                            <td colspan="3">Lists cluster types.</td>
                        </tr>
                    </tbody>
                </informaltable>
                <para>This operation does not require a request
                    body.</para>
                <para>The following examples show the cURL request and
                    the corresponding response for the List Flavors
                    operation. </para>
                <example>
                    <title>cURL List Cluster Types Request:
                        JSON</title>
                    <programlisting language="bash">curl -i -X GET https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/types -d \
-H "X-Auth-Token: <emphasis role="bold">yourAuthToken</emphasis>" \
-H "Accept: application/json" \
-H "Content-type: application/json"</programlisting>
                </example>
                <example>
                    <title>List Cluster Types Response: JSON</title>
                    <programlisting language="json" >
 
        { "types":
          [
            {
              "id": "HADOOP_HDP1_1",
              "links": [
                { "href": "http://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/types/HADOOP_HDP1_1",
                  "rel": "self"
                },
                { "href": "http://dfw.bigdata.api.rackspacecloud.com/1234/types/HADOOP_HDP1_1",
                  "rel": "bookmark"
                }
              ],
              "name": "Hadoop (HDP 1.1)"
            }
          ]
        }
        
        
</programlisting>
                </example>
            </section>
        <section xml:id="createCluster">
            <title>Creating a Cluster  </title><para xmlns:wadl="http://wadl.dev.java.net/2009/02">The Create Cluster
                    operation creates a new Hadoop cluster. After you
                    submit the operation, it takes a few minutes for
                    the service to build and configure your
                    cluster.</para>
            <informaltable rules="all"
                xmlns:wadl="http://wadl.dev.java.net/2009/02">
                <thead>
                    <tr align="center">
                        <td colspan="1">Verb</td>
                        <td colspan="2">URI</td>
                        <td colspan="3">Description</td>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td colspan="1">&POST;</td>
                        <td colspan="2">/clusters</td>
                        <td colspan="3">Creates a new cluster.</td>
                    </tr>
                </tbody>
            </informaltable><para>The following example creates a 5-data-node Hadoop cluster with
                    the name <code>slice</code>.</para>
            <para>The value of the <code>clusterType</code> attribute
                    must be HADOOP_HDP1_1. The <code>flavorID</code>
                    attribute specifies the RAM, virtual CPUs, and
                    disk storage associated with each data node in
                    your Hadoop cluster. </para>
            <para xmlns:wadl="http://wadl.dev.java.net/2009/02">The
                    following examples show the cURL request and
                    corresponding response for Create Cluster.</para>
            <remark>Reviewer: Please provide appropriate cURL to
                produce the response given in this example.</remark>
            <example xmlns:wadl="http://wadl.dev.java.net/2009/02">
                <title>cURL Create Cluster Request: JSON</title>
                <programlisting language="bash">curl -i -X POST https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/clusters -d \
'{"cluster":
      { "name": "slice",
        "clusterType": "HADOOP_HDP1_1",
        "flavorId": "4fba3bca-7c76-11e2-b737-beeffa00040e",
        "nodeCount": 5
      }
    }' \          
-H "X-Auth-Token: <emphasis role="bold">yourAuthToken</emphasis>" \
-H "Accept: application/json" \
-H "Content-type: application/json"                 </programlisting>
              
            </example>
            <example xmlns:wadl="http://wadl.dev.java.net/2009/02">
                <title>Create Cluster Response: JSON</title>
                <programlisting language="json" >    {"cluster":
      { "id": "db478fc1-2d86-4597-8010-cbe787bbbc41",
        "created": "2012-12-27T10:10:10Z",
        "updated": "",
        "name": "slice",
        "clusterType": "HADOOP_HDP1_1",
        "flavorId": "4fba3bca-7c76-11e2-b737-beeffa00040e",
        "nodeCount": 5,
        "status": "BUILD",
        "links": [
          { "rel": "self",
            "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41"
          },
          { "rel": "bookmark",
            "href": "https://dfw.bigdata.api.rackspacecloud.com/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41"
          }
        ],
      }
    }</programlisting>
 
            </example>   
        </section>
            <section xml:id="List-Clusters-d1e004">
                <title>Listing Clusters</title>
                <para>You use the List Clusters operation to find the
                    available clusters for your account.</para>
                <informaltable rules="all">
                    <thead>
                        <tr align="center">
                            <td colspan="1">Verb</td>
                            <td colspan="2">URI</td>
                            <td colspan="3">Description</td>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td colspan="1">&GET;</td>
                            <td colspan="2">/clusters</td>
                            <td colspan="3">Lists all clusters for
                                your account.</td>
                        </tr>
                    </tbody>
                </informaltable>
                <para>This operation does not require a request
                    body.</para>
                <para>The following examples show the cURL request and
                    the corresponding response for the List Flavors
                    operation. </para>
                <example>
                    <title>cURL List Clusters Request: JSON</title>
                    <programlisting language="bash">curl -i -X GET https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/clusters -d \
-H "X-Auth-Token: <emphasis role="bold">yourAuthToken</emphasis>" \
-H "Accept: application/json" \
-H "Content-type: application/json"</programlisting>
                </example>
                <example>
                    <title>List Clusters Response: JSON</title>
                    <programlisting language="json">
        {"clusters":
          [
            { "id": "db478fc1-2d86-4597-8010-cbe787bbbc41",
              "name": "slice",
              "created": "2012-12-27T10:10:10Z",
              "updated": "2012-12-27T10:15:10Z", 
              "clusterType": "HADOOP_HDP1_1",
              "flavorId": "4fba3bca-7c76-11e2-b737-beeffa00040e",
              "nodeCount": 5,
              "postInitScriptStatus": "SUCCEEDED",
              "progress": 1.0,
              "status": "ACTIVE",
              "links": [
                { "rel": "self",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41"
                },
                { "rel": "bookmark",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41"
                }
              ],
            },
            { "id": "ac111111-2d86-4597-8010-cbe787bbbc41",
              "name": "real",
              "created": "2012-12-27T10:10:10Z",
              "updated": "2012-12-27T10:15:10Z", 
              "clusterType": "HBASE_HDP1_1",
              "flavorId": "518400b2-7c76-11e2-b737-beeffa00040e",
              "nodeCount": 20,
              "postInitScriptStatus": null,
              "progress": 1.0,
              "status": "ACTIVE",
              "links": [
                { "rel": "self",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/clusters/ac111111-2d86-4597-8010-cbe787bbbc41"
                },
                { "rel": "bookmark",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/1234/clusters/ac111111-2d86-4597-8010-cbe787bbbc41"
                }
              ],
            },
          ]
        }
        
</programlisting>
                </example>
            </section>
        <section xml:id="viewDetails">
            <title>Viewing Details</title>
            <para>The Get Node Details operation lists all server
                    nodes for the specified cluster.</para>
            <informaltable rules="all">
                <thead>
                    <tr align="center">
                        <td colspan="1">Verb</td>
                        <td colspan="2">URI</td>
                        <td colspan="3">Description</td>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td colspan="1">&GET;</td>
                        <td colspan="2">/clusters/{id}/nodes</td>
                        <td colspan="3">Lists all nodes for a
                                cluster.</td>
                    </tr>
                </tbody>
            </informaltable>
            <para>In the following example, the cluster has a name
                    node, a gateway node, and two data nodes. Each
                    node has a private IP address, which is used for
                    backend (Hadoop) data transfer, and a public IP
                    address, which enables you to access it over the
                    public Internet. Use the gateway node to do your
                    work, including using tools such as Apache Pig and
                    Apache Hive. In the example, you can remotely SSH
                    into the gateway node over the IP address by using
                    the user name and password that you added to your
                    profile during setup.</para>
            <para>This operation does not have a request body.</para>
            <para>The following examples show the cURL request and
                    corresponding response for listing all nodes for a
                    cluster.</para>
            <remark>Reviewer: Please provide appropriate cURL to
                produce the response given in this example.</remark>
            <example>
                <title>cURL List Cluster Nodes Request: JSON</title>
                <programlisting language="bash">curl -i -X GET https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/clusters/ac111111-2d86-4597-8010-cbe787bbbc41/nodes -d \
-H "X-Auth-Token: <emphasis role="bold">yourAuthToken</emphasis>" \
-H "Accept: application/json" \
-H "Content-Type: application/json"</programlisting>
            </example>
            <example>
                <title>List Cluster Nodes Response: JSON</title>
                <programlisting language="json">        {"nodes":
          [ 
            { "id": "000",
              "created": "2012-12-27T10:10:10Z", 
              "role": "NAMENODE",
              "name": "NAMENODE-1", 
              "status": "ACTIVE",
              "addresses": {
                "public": [{ "addr": "168.x.x.3", "version": 4 }],
                "private": [{ "addr": "10.x.x.3", "version": 4}]
              },
              "services": [
                { "name": "namenode" },
                { "name": "jobtracker" },
                { "name": "ssh",
                  "uri": "ssh://user@168.x.x.3"
                }
              ],
              "links": [
                { "rel": "self",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41/nodes/000"
                },
                { "rel": "bookmark",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41/nodes/000"
                }
              ],
            },
            { "id": "aaa",
              "role": "GATEWAY",
              "name": "GATEWAY-1", 
              "status": "ACTIVE",
              "addresses": {
                "public": [{ "addr": "168.x.x.4", "version": 4 }],
                "private": [{ "addr": "10.x.x.4", "version": 4}]
              },
              "services": [
                { "name": "pig" },
                { "name": "hive" },
                { "name": "ssh",
                  "uri": "ssh://user@168.x.x.4"
                },
                { "name": "status",
                  "uri": "http://10.x.x.4"
                },
                { "name": "hdfs-scp",
                  "uri": "scp://user@168.x.x.4:9022"
                }
              ],
              "links": [
                { "rel": "self",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41/nodes/aaa"
                },
                { "rel": "bookmark",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41/nodes/aaa"
                }
              ],
            },
            { "id": "bbb",
              "role": "DATANODE",
              "name": "DATANODE-1",  
              "status": "ACTIVE",
              "addresses": {
                "public": [{ "addr": "168.x.x.5", "version": 4 }],
                "private": [{ "addr": "10.x.x.5", "version": 4}]
              },
              "services": [
                { "name": "datanode" },
                { "name": "tasktracker" },
                { "name": "ssh",
                  "uri": "ssh://user@168.x.x.5"
                }
              ],
              "links": [
                { "rel": "self",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41/nodes/bbb"
                },
                { "rel": "bookmark",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41/nodes/bbb"
                }
              ],
            },
            { "id": "ccc",
              "role": "DATANODE",
              "name": "DATANODE-2",  
              "status": "ACTIVE",
              "addresses": {
                "public": [{ "addr": "168.x.x.6", "version": 4 }],
                "private": [{ "addr": "10.x.x.6", "version": 4}]
              },
              "services": [
                { "name": "datanode" },
                { "name": "tasktracker" },
                { "name": "ssh",
                  "uri": "ssh://user@168.x.x.6"
                }
              ],
              "links": [
                { "rel": "self",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41/nodes/ccc"
                },
                { "rel": "bookmark",
                  "href": "https://dfw.bigdata.api.rackspacecloud.com/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41/nodes/ccc"
                }
              ],
            } 
          ]
        }</programlisting>
 
            </example> <para/>
        </section>
        <section xml:id="resizeClusters">
            <title>Resizing Clusters</title>
            <para>You can increase the size of an existing cluster by
                    using the <code>resize</code> command. Downsizing
                    clusters is not currently supported. </para>
            <informaltable rules="all">
                <thead>
                    <tr align="center">
                        <td colspan="1">Verb</td>
                        <td colspan="2">URI</td>
                        <td colspan="3">Description</td>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td colspan="1">&POST;</td>
                        <td colspan="2">/clusters/{clusterId}/action</td>
                        <td colspan="3">Resizes a cluster.</td>
                    </tr>
                </tbody>
            </informaltable>
            <para>The following example resizes the previously created
                    cluster, named <code>slice</code>, to include 10
                    data nodes. When you use the <code>resize</code>
                    command, you specify the total number of data
                    nodes that you want the cluster to have, not just
                    the number of nodes to add. In the example, 10 is
                    the total number of nodes that the cluster will
                    have after the command is run. After you initiate
                    the <code>resize</code> operation, you can use the
                    List Cluster Nodes operation to confirm that your
                    cluster has been resized.</para>
            <remark>Reviewer: Please provide appropriate cURL to
                produce the response given in this example.</remark>
            <para>The following examples show the cURL request and
                    corresponding response to resize a cluster.</para>
            <example>
                <title>cURL Resize Cluster Request: JSON</title>
                <programlisting language="bash">curl -i -X POST https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/clusters/<emphasis role="bold">your_cluster_ID</emphasis>/action -d \
'{"resize":{ "nodeCount": 10 }} ' \
-H "Accept: application/json" \
-H "X-Auth-Token:<emphasis role="bold">yourAuthToken</emphasis>" \
-H "Content-Type: application/json"  
</programlisting>
            </example>
            <example>
                <title>Resize Cluster Response: JSON</title>
                <programlisting language="json" >        {"cluster":
          { "id": "db478fc1-2d86-4597-8010-cbe787bbbc41",
            "created": "2012-12-27T10:10:10Z",
            "updated": "2012-12-27T16:20:10Z",
            "name": "slice",
            "clusterType": "HADOOP_HDP1_1",
            "flavorId": "4fba3bca-7c76-11e2-b737-beeffa00040e",
            "nodeCount": 10,
            "status": "RESIZING",
            "links": [
              { "rel": "self",
                "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41"
              },
              { "rel": "bookmark",
                "href": "https://dfw.bigdata.api.rackspacecloud.com/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41"
              }
            ],
          }
        } </programlisting>
     
            </example>
        </section>
        <section xml:id="deleteCluster">
            <title>Deleting Clusters</title>
            <para>Use the Delete Cluster operation to remove unused
                    Hadoop clusters. This operation deletes any
                    servers associated with the cluster and any data
                    stored in the cluster.</para>
            <para>You cannot delete clusters that are in the process
                    of being created or resized.</para>
            <informaltable rules="all">
                <thead>
                    <tr align="center">
                        <td colspan="1">Verb</td>
                        <td colspan="2">URI</td>
                        <td colspan="3">Description</td>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td colspan="1">&DELETE;</td>
                        <td colspan="2">/clusters/{id}</td>
                        <td colspan="3">Deletes a cluster.</td>
                    </tr>
                </tbody>
            </informaltable>
            <para>The following examples show the cURL request and
                    corresponding response to delete a cluster.</para>
            <remark>Reviewer: Please provide appropriate cURL to
                    produce the response given in this example. </remark>
            <example>
                <title>cURL Delete Cluster Request: JSON</title>
                <programlisting language="bash">curl -i -X DELETE https://dfw.bigdata.api.rackspacecloud.com/v1.0/<emphasis role="bold">yourAccountID</emphasis>/clusters/<emphasis role="bold">your_cluster_ID</emphasis> -d \
-H "Accept: application/json" \
-H "X-Auth-Token:<emphasis role="bold">yourAuthToken</emphasis>" \
-H "Content-Type: application/json"</programlisting>
            </example>
            <example>
                <title>Delete Cluster Response: JSON</title>
                <programlisting language="json">Status: 202 Accepted
        Date: Mon, 06 Aug 2012 21:54:21 GMT
        Content-Type: application/json
        {"cluster":
          { "id": "db478fc1-2d86-4597-8010-cbe787bbbc41",
            "created": "2012-12-27T10:10:10Z",
            "updated": "2012-12-27T20:14:10Z",
            "name": "slice",
            "clusterType": "HADOOP_HDP1_1",
            "flavorId": "4fba3bca-7c76-11e2-b737-beeffa00040e",
            "nodeCount": 5,
            "status": "DELETING",
            "links": [
              { "rel": "self",
                "href": "https://dfw.bigdata.api.rackspacecloud.com/v1.0/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41"
              },REST
              { "rel": "bookmark",
                "href": "https://dfw.bigdata.api.rackspacecloud.com/1234/clusters/db478fc1-2d86-4597-8010-cbe787bbbc41"
              }
            ]
          }
        }
</programlisting>
             
            </example>
        </section>
    </section>

    </chapter>
    <chapter xml:id="Using_Python_lavaclient">
        <title>Using the Python Lava Client</title>
        <para>This chapter explains how to start using the Python Lava
            client (python-lavaclient), which is a convenient Python
            binding and command-line client that uses the RESTful
            API.</para>
        <section xml:id="setting_Up">
            <title>Setting Up</title>
            <para>This section describes prerequisites, installation,
                and setup for the python-lavaclient.</para>
            <section xml:id="Prerequisites">
                <title>Prerequisites</title>
                <para>Following are the requirements for using
                    python-lavaclient:</para>
                <itemizedlist>
                    <listitem>
                        <para>Computer running Linux or Mac OS
                            X</para>
                    </listitem>
                    <listitem>
                        <para>Python 2.7.2 or later</para>
                    </listitem>
                    <listitem>
                        <para>Rackspace Cloud account and access to
                            Rackspace Cloud Big Data</para>
                    </listitem>
                </itemizedlist>
            </section>
            <section xml:id="installing_Client">
                <title>Installing the Client</title>
                <para>Perform the following steps to install the
                    client. </para>
                <orderedlist>
                    <listitem>
                        <para>Download python-lavaclient to your
                            computer from <link
                                xlink:href="http://bit.ly/cbd-client"
                                >http://bit.ly/cbd-client</link>.</para>
                    </listitem>
                    <listitem>
                        <para>Extract the archive by using the
                            following command: </para>
                        <programlisting>$ tar xvzf python-lava-0.8.6.tgz</programlisting>
                        <para>A python-lavaclient folder is created.
                        </para>
                    </listitem>
                    <listitem>
                        <para>Open the folder and install the client
                            by using the following commands:</para>
                        <programlisting>$ cd python-lavaclient 
$ sudo python setup.py install</programlisting>
                    </listitem>
                    <listitem>
                        <para>Run the help command to ensure that the
                            client has been installed correctly and
                            note the usage information: </para>
                        <programlisting>$ lava help</programlisting>
                    </listitem>
                </orderedlist>
            </section>
            <section xml:id="authenticate_Against_API">
                <title>Authenticating Your Session</title>
                <para>Before you can start using Rackspace Cloud Big
                    Data, you need to authenticate your session. To
                    complete this step, you need your Cloud user name
                    and API key.</para>
                <orderedlist>
                    <listitem>
                        <para>Run the <code>auth</code> command. When
                            prompted, enter your user name and API
                            key. </para>
                        <programlisting>$ lava auth
Username: &lt;enter your Cloud username>
API Key: &lt;enter your API Key></programlisting>
                        <para>If the command runs successfully, your
                            authentication token and access endpoint
                            are displayed, as shown in the following
                            example.</para>
                        <example>
                            <title>Authentication Response Using CLI
                                Utility</title>
                            <programlisting>AUTH_TOKEN=692c2a14-39ad-4ee0-991d-06cd7331f3ca
LAVA_API_URL=https://dfw.bigdata.api.rackspacecloud.com/v1.0/12345</programlisting>
                        </example>
                    </listitem>
                    <listitem>
                        <para>Export the AUTH_TOKEN and LAVA_API_URL
                            environment variables as shown in the
                            following example.</para>
                        <example>
                            <title>Export Environment
                                Variables</title>
                            <programlisting>$ export AUTH_TOKEN=692c2a14-39ad-4ee0-991d-06cd7331f3ca
$ export LAVA_API_URL=https://dfw.bigdata.api.rackspacecloud.com/v1.0/12345</programlisting>
                        </example>
                        <note>
                            <para>The <code>export</code> commands are
                                valid only for the current session.
                                You need to rerun the
                                   <code>export</code> commands if,
                                for example, you create a new console
                                window.</para>
                        </note>
                    </listitem>
                    <listitem>
                        <para>To confirm that the client is running,
                            run the list command. The first time that
                            you run this command, it does not return
                            any output. </para>
                        <programlisting>$ lava list</programlisting>
                    </listitem>
                </orderedlist>
            </section>
            <section xml:id="setting_Up_Profile">
                <title>Setting Up a Profile</title>
                <para>Before you can create Hadoop clusters, you must
                    create a profile that consists of a user name and
                    password. To create a profile, use the
                        <code>create-profile</code> command as shown
                    in the following example.  <note
                        xmlns:wadl="http://wadl.dev.java.net/2009/02">
                        <para>Your Cloud Big Data profile is different
                            from your cloud account. Your profile has
                            the following characteristics and
                            requirements:</para>
                        <itemizedlist>
                            <listitem>
                                <para>A profile is the configuration
                                   for the administration and login
                                   account for the cluster.</para>
                            </listitem>
                            <listitem>
                                <para>Only one profile is allowed for
                                   each user or account.</para>
                            </listitem>
                            <listitem>
                                <para>Any updates or additions
                                   override the existing profile.
                                </para>
                            </listitem>
                        </itemizedlist>
                    </note></para>
                <para>In this example, a profile is created with the
                    user name <code>joe</code> and the password
                        <code>Fool1234!</code>. </para>
                <example>
                    <title>Creating a Profile</title>
                    <programlisting>$ lava create-profile joe Foo1234!</programlisting>
                </example>
                <para>A password must be at least eight characters,
                    with a mixture of uppercase, lowercase, numeric,
                    and special characters.</para>
                <para>After you create your profile, every Hadoop
                    server that you provision by using the API will
                    have a local user account with the specified name
                    and password. This allows you to remotely SSH into
                    a server to transfer data, run or troubleshoot
                    jobs, and so on.</para>
                <para>To confirm that your profile was successfully
                    created, run the <code>get-profile</code>
                    command.</para>
                <example>
                    <title>Retrieve a Profile by Using the get-profile
                        Command</title>
                    <programlisting>$ lava get-profile
+-------------+-----------+-------------+
| Username | User ID | Tenant ID |
+-------------+---------+---------------+
| joe | 118091 | 12345 |
+-------------+-----------+-------------+</programlisting>
                </example>
            </section>
            <section xml:id="view_Resource_Limits">
                <title>Viewing Resource Limits</title>
                <para>The use of Rackspace Cloud Big Data is subject
                    to resource limits. You can view the limits
                    associated with your account by using the
                        <code>quotas</code> command. </para>
                <example>
                    <title>View Resource Limits by Using the quotas
                        Command</title>
                    <programlisting>$ lava quotas
+-------------+-----------------+-------------+-----------------+-----------+---------------+------------+----------------+
| Nodes Limit | Nodes Remaining | VCPUs Limit | VCPUs Remaining | RAM Limit | Ram Remaining | Disk Limit | Disk Remaining |
+-------------+-----------------+-------------+-----------------+-----------+---------------+------------+----------------+
| 5 | 5 | 5 | 5 | 25600 | 25600 | 2048 | 2048 |
+-------------+-----------------+-------------+-----------------+-----------+---------------+------------+----------------+</programlisting>
                </example>
            </section>
        </section>
        <section xml:id="create_Manage_Clusters">
            <title>Creating and Managing Hadoop Clusters</title>
            <para>This section explains how to create and manage
                Hadoop clusters by using Rackspace Cloud Big Data
                through the python-lavaclient. After you complete this
                section, you will have a working Apache Hadoop
                cluster.</para>
            <section xml:id="boot_Hadoop_Cluster">
                <title>Provisioning a Hadoop Cluster</title>
                <para>Use the <code>boot</code> command to provision a
                    Hadoop cluster. The following example creates a
                    3-data-node Hadoop cluster with the name
                        <code>foo</code>.</para>
                <example>
                    <title>Provision a Cluster by Using the boot
                        Command </title>
                    <programlisting>$ lava boot --type=HADOOP_HDP1_1 --flavor=4fba3bca-7c76-11e2-b737-beeffa00040e --count=3 foo
+----------+--------------------------------------+
| Property | Value |
+----------+--------------------------------------+
| Id | 4820deb2-6212-44f9-b92f-979fe723ffb8 |
| Name | foo |
| Status | BUILD |
| Nodes | 3 |
| Type | HADOOP_HDP1_1 |
| Flavor | 4fba3bca-7c76-11e2-b737-beeffa00040e |
+----------+--------------------------------------+</programlisting>
                </example>
                <para>The <code>type</code> argument must specify
                    HADOOP_HDP1_1. The <code>flavor</code> argument
                    specifies the RAM, virtual CPUs, and disk storage
                    associated with each data node in your Hadoop
                    cluster. You can enumerate the flavors and
                    associated resources by using the
                        <code>flavor-list</code> command, as shown in
                    the following example.</para>
                <example>
                    <title>List Flavors and Associated Resources by
                        Using the flavor-list Command </title>
                    <programlisting>$ lava flavor-list
+--------------------------------------+------------------------+-------+------+------+
| Id | Name | Mem | Disk | CPUs |
+--------------------------------------+------------------------+-------+------+------+
| 4fba3bca-7c76-11e2-b737-beeffa00040e | Tiny Hadoop Instance | 5120 | 256 | 1 |
| 518400b2-7c76-11e2-b737-beeffa00040e | Small Hadoop Instance | 8192 | 512 | 2 |
| 5413894c-7c76-11e2-b737-beeffa00040e | Medium Hadoop Instance | 15360 | 1024 | 4 |
| 559dd2b8-7c76-11e2-b737-beeffa00040e | Large Hadoop Instance | 27648 | 2048 | 8 |
+--------------------------------------+------------------------+-------+------+------+</programlisting>
                </example>
                <para>After you run the <code>boot</code> command, it
                    takes a few minutes for the service to build and
                    configure your cluster. Use the <code>list</code>
                    command to query the build status of your
                    cluster.</para>
                <example>
                    <title>Query the Status of a Cluster by Using the
                        list Command</title>
                    <programlisting>$ lava list
+--------------------------------------+------+--------+-------+---------------+
| Id | Name | Status | Nodes | Type |
+--------------------------------------+------+--------+-------+---------------+
| 4820deb2-6212-44f9-b92f-979fe723ffb8 | foo | ACTIVE | 3 | HADOOP_HDP1_1 |
+--------------------------------------+------+--------+-------+---------------+</programlisting>
                </example>
                <para>When your cluster is ready, the value of
                        <code>Status</code> is
                    <code>ACTIVE</code>.</para>
            </section>
            <section xml:id="viewing_Details">
                <title>Viewing Details</title>
                <para>Use the <code>show</code> and <code>nodes</code>
                    commands to query your cluster, as shown in the
                    following example.</para>
                <example>
                    <title>Query the Details of a Cluster by Using the
                        show and nodes Commands</title>
                    <programlisting>$ lava show foo
+----------+--------------------------------------+
| Property | Value |
+----------+--------------------------------------+
| Id | 4820deb2-6212-44f9-b92f-979fe723ffb8 |
| Name | foo |
| Status | ACTIVE |
| Nodes | 3 |
| Type | HADOOP_HDP1_1 |
| Flavor | 4fba3bca-7c76-11e2-b737-beeffa00040e |
+----------+--------------------------------------+
$ lava nodes foo
+--------------------------------------+--------------+----------+--------+----------------+----------------+
| Id | Name | Role | Status | Public IP | Private IP |
+--------------------------------------+--------------+----------+--------+----------------+----------------+
| f530a9f1-79a8-4378-bf2a-b7f7e0c2bdd3 | NAMENODE-88 | NAMENODE | ACTIVE | 166.78.132.85 | 10.190.240.88 |
| 87a1bf57-daa6-4dd5-b711-f428cc44fe74 | DATANODE-182 | DATANODE | ACTIVE | 166.78.132.228 | 10.190.240.228 |
| 82136cd0-2c98-4c76-9224-54aef9942de3 | DATANODE-183 | DATANODE | ACTIVE | 166.78.132.84 | 10.190.240.84 |
| 8a9929ed-9ba3-4a22-b5a7-8a36a81179a6 | DATANODE-184 | DATANODE | ACTIVE | 166.78.132.88 | 10.190.240.85 |
| 55ed5001-fdc3-44f1-a82c-97532007f3b3 | GATEWAY-89 | GATEWAY | ACTIVE | 166.78.132.227 | 10.190.240.227 |
+--------------------------------------+--------------+----------+--------+----------------+----------------+
</programlisting>
                </example>
                <para>The example shows that the cluster has the
                    following nodes:</para>
                <itemizedlist>
                    <listitem>
                        <para>One name node</para>
                    </listitem>
                    <listitem>
                        <para>One gateway node</para>
                    </listitem>
                    <listitem>
                        <para>Three data nodes</para>
                    </listitem>
                </itemizedlist>
                <para>Each server node has the following IP addresses:<itemizedlist>
                        <listitem>
                            <para>A private IP address that is used
                                for backend (Hadoop) data
                                transfers</para>
                        </listitem>
                        <listitem>
                            <para>A public IP address that allows you
                                to access the server over the public
                                Internet</para>
                        </listitem>
                    </itemizedlist>Use the gateway node to do your
                    work, including using tools such as Pig and Hive.
                    In the example, you can remotely SSH into the
                    gateway node over IP address 166.78.132.227 by
                    using the user name and password that you used to
                    create your profile during setup.</para>
            </section>
            <section xml:id="resize_Cluster">
                <title>Resizing Clusters</title>
                <para>You can increase the size of an existing cluster
                    by using the <code>resize</code> command.
                    Downsizing clusters is not currently supported.
                    The following example resizes the previously
                    created cluster <code>foo</code> to include 5 data nodes.<example>
                        <title>Increase Cluster Size by Using the
                            resize Command</title>
                        <programlisting>$ lava resize --count=5 foo
</programlisting>
                    </example></para>
                <para>When you use the <code>resize</code> command,
                    you specify the total number of data nodes that
                    you want the cluster to have, not just the number
                    of nodes to add. In the example, 5 is the total
                    number of nodes that the cluster will have after
                    the command is run.  After you initiate the
                        <code>resize</code> operation, use the
                        <code>show</code> or <code>list</code>
                    commands to confirm that your cluster has been
                    resized. </para>
            </section>
            <section xml:id="delete_Cluster">
                <title>Deleting Clusters</title>
                <para>Use the <code>delete</code> command to remove
                    unused Hadoop clusters. This command deletes any
                    servers associated with the cluster and any data
                    stored in the cluster.</para>
                <example>
                    <title>Remove Clusters by Using the delete
                        Command</title>
                    <programlisting>$ lava delete foo
</programlisting>
                </example>
                <note>
                    <para>You cannot delete clusters that are in the
                        process of being created or resized. </para>
                </note>
            </section>
        </section>
        

<!-- Possible additions from CBD wiki getting started  
            
        <section xml:id="word_Count_Application">
            <title>Running a Sample Word Count Application</title>
            <para>This section explains how to run WordCount, which is
                a sample MapReduce application that ships with the
                Apache Hadoop distribution. The application simply
                counts the occurrence of every word in the provided
                text. </para>
            <para>Prior to working through this section, be sure you
                have worked through the sections <xref
                    linkend="setting_Up"/> and <xref
                    linkend="create_Manage_Clusters"/> to set up a
                3-node cluster.</para>
            <section xml:id="copy_Input_Data">
                <title>Copying Input Data</title>
                <para>The sample application requires some input text
                    data to work against. A real application normally
                    runs against an input folder containing many large
                    files. For this exercise, let's assume that you
                    have downloaded <citetitle>The Complete Works of
                        William Shakespeare</citetitle> (a single
                    file) onto your computer or laptop. </para>
                <para>You can use several different techniques to
                    transfer data into Hadoop. For convenience,
                    Rackspace provides a secure copy protocol (SCP)
                    service that you can use to upload and access data
                    in Hadoop. The SCP service runs on the gateway
                    node that is automatically provisioned by
                    Rackspace Cloud Big Data. The SCP service reads
                    and writes directly into the Hadoop File System,
                    not the local disk on the gateway node. </para>
                <para>Use the nodes command to determine the IP
                    address of the gateway node. In the example below,
                    the gateway node has the public IP address
                    166.78.132.227.</para>
                <example>
                    <title>Nodes Command to Determine IP
                        Address</title>
                    <programlisting>$ lava nodes foo
+_______________________________________+______________+___________+__________+___________+_________________+
| Id | Name | Role | Status | Public IP | Private IP |
+_______________________________________+______________+___________+__________+___________+_________________+
| f530a9f1-79a8-4378-bf2a-b7f7e0c2bdd3 | NAMENODE-88 | NAMENODE | ACTIVE | 166.78.132.85 | 10.190.240.88 |
| 87a1bf57-daa6-4dd5-b711-f428cc44fe74 | DATANODE-182 | DATANODE | ACTIVE | 166.78.132.228 | 10.190.240.228 |
| 82136cd0-2c98-4c76-9224-54aef9942de3 | DATANODE-183 | DATANODE | ACTIVE | 166.78.132.84 | 10.190.240.84 |
| 8a9929ed-9ba3-4a22-b5a7-8a36a81179a6 | DATANODE-184 | DATANODE | ACTIVE | 166.78.132.88 | 10.190.240.85 |
| 55ed5001-fdc3-44f1-a82c-97532007f3b3 | GATEWAY-89 | GATEWAY | ACTIVE | 166.78.132.227 | 10.190.240.227 |
+_______________________________________+______________+___________+__________+___________+_________________+
</programlisting>
                </example>
                <para>Run the following commands to create a folder on
                    the Hadoop file system (called
                        <parameter>input</parameter>) and upload your
                    input text data into it.</para>
                <programlisting>$ ssh joe@166.78.132.227 mkdir input</programlisting>
                <programlisting>$ scp -P 9022 input.txt joe@166.78.132.227:input</programlisting>
                <para>Your Hadoop cluster now has some input data on
                    it in order for you to work with.</para>
                <note>
                    <para>The SCP service runs on port 9022. Using the
                        default port (22)  ends up copying data to the
                        local disk on the gateway server (not the
                        Hadoop file system).</para>
                </note>
            </section>
            <section xml:id="run_WordCount">
                <title>Running WordCount</title>
                <para>To run the sample application, you need to SSH
                    into the gateway in your cluster. After you are
                    logged in, run the following command to check out
                    the sample applications that ship with Apache
                    Hadoop.</para>
                <programlisting>$ hadoop jar /usr/lib/hadoop/hadoop-examples.jar</programlisting>
                <para>The resulting output should look like the
                    following example. </para>
                <screenshot>
                    <title>Example of Sample Applications with Apache
                        Hadoop</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata format="svg" align="center" scale="50"
                                fileref="../resources/samples/GSG_CLI_Screen2.png "/>
                        </imageobject>
                    </mediaobject>
                </screenshot>
                <para>In particular, the WordCount application
                    requires an input folder (already created) and an
                    output folder. Launch the application using the
                    command line shown below.</para>
                <programlisting>$ hadoop jar /usr/lib/hadoop/hadoop-examples.jar wordcount input output</programlisting>
                <para/>
                <para>The resulting output should look like the
                    following example.<figure>
                        <title>Output from Creation of folders</title>
                        <mediaobject>
                            <imageobject>
                                <imagedata format="svg" align="center" scale="50"
                                   fileref="../resources/samples/GSG_CLI_Screen1.png"/>
                            </imageobject>
                        </mediaobject>
                    </figure></para>
                <para>After the command completes successfully, the
                    results are written to a folder called
                        <parameter>output</parameter> on the Hadoop
                    file system. You can view the output folder using
                    your FTP client, or by running the following
                    command.</para>
                <programlisting>$ hadoop fs âls output</programlisting>
                <para>At this point, you have used the hadoop script
                    to launch the sample application. This is a
                    general-purpose utility for running a variety of
                    Hadoop-related commands. Look at <citetitle>Hadoop
                        Commands Guide</citetitle> for a more detailed
                    explanation. </para>
                <para>Going back to our command line, the
                        <command>jar</command> subcommand is used to
                    launch a MapReduce bundled in a jar file: </para>
                <programlisting>hadoop jar /usr/lib/hadoop/hadoop-examples.jar wordcount input output</programlisting>
                <para> The argument following the
                        <command>jar</command> subcommand is the full
                    path for the Java package that contains all the
                    sample applications. The path is followed by the
                    application to run, namely
                        <parameter>wordcount</parameter>. The input
                    and output parameters are specific to the
                    WordCount application, and specify the input and
                    output folders respectively.</para>
            </section>
        </section>
        <section xml:id="python_Hadoop_Application">
            <title>Building a Python Hadoop Application</title>
            <para>This section explains how to use process data in
                Hadoop using the Python programming language. Since
                Hadoop's core components are Java-based, it natively
                only supports MapReduce applications written in Java.
                This presents a problem for developers who wish to
                build data processing applications using an
                alternative programming language (such as Python or
                Ruby). Luckily, Hadoop includes a streaming utility,
                which allows any executable to serve as a mapper or
                reducer. You can find more details about the tool in
               <link
                    xlink:href="http://wiki.apache.org/hadoop/HadoopStreaming/"
                    >Hadoop Streaming Wiki/</link>.
                The simple Python MapReduce application in this
                section relies on streaming. </para>
            <para>Prior to working through this section, be sure to
                read <xref linkend="setting_Up"/> and <xref
                    linkend="create_Manage_Clusters"/> to set up a
                3-node cluster. Also, take a look at <xref
                    linkend="run_WordCount"/> in order to learn how to
                copy data into Hadoop and run the packaged (Java)
                WordCount application.</para>
            <section xml:id="code_Python_App">
                <title>Coding the Python application</title>
                <para>SSH into your gateway node and use the following
                    code snippets to create your Python mapper and
                    reducer. </para>
                <para>The code for the mapper function is below. Use
                    your favorite editor to save it as mapper.py on
                    the gateway node. </para>
                <note>
                    <para>Python cares about indentation, so be sure
                        to maintain it when you copy and paste the
                        code using your editor.</para>
                </note>
                <programlisting>#!/usr/bin/env python

import sys

# input comes from STDIN (standard input)
for line in sys.stdin:

      # remove leading and trailing whitespace
      line = line.strip()

      # split the line into words
      words = line.split()

           # increase counters
           for word in words:
           # write the results to STDOUT (standard output);
           # what we output here will be the input for the
           # Reduce step, i.e. the input for reducer.py
           #
           # tab-delimited; the trivial word count is 1
           print '%s\t%s' % (word, 1)</programlisting>
                <para>Similarly, save the following code as <emphasis
                        role="bold">reducer.py</emphasis> on your
                    gateway node.</para>
                <programlisting>#!/usr/bin/env python

from operator import itemgetter
import sys

current_word = None
current_count = 0
word = None

# input comes from STDIN
for line in sys.stdin:

      # remove leading and trailing whitespace
      line = line.strip()

      # parse the input we got from mapper.py
      word, count = line.split('\t', 1)

      # convert count (currently a string) to int
      try:

          count = int(count)

      except ValueError:

          # count was not a number, so silently
          # ignore/discard this line
          continue

      # this IF-switch only works because Hadoop sorts map output
      # by key (here: word) before it is passed to the reducer
      if current_word == word:

            current_count += count

      else:

           if current_word:

                 # write result to STDOUT
                 print '%s\t%s' % (current_word, current_count)

           current_count = count
           current_word = word

# do not forget to output the last word if needed!
if current_word == word:

      print '%s\t%s' % (current_word, current_count)</programlisting>
            </section>
            <section xml:id="run_Python_App">
                <title>Running the Python application</title>
                <para>Use the same input folder for this application
                    that you created for <xref linkend="run_WordCount"
                    />. Be sure to follow the steps to copy the data,
                    if you have not already. To launch the job, run
                    the following command line from the gateway
                    node.</para>
                <programlisting>hadoop jar /usr/lib/hadoop/contrib/streaming/hadoop-streaming-0.20.2-cdh3u2.jar  -input input -output OUTPUT1 -mapper mapper.py -reducer reducer.py -file mapper.py  -file reducer.py</programlisting>
                <para>When the job completes, the results is in an
                    HDFS folder called <emphasis role="bold"
                        >OUTPUT1</emphasis>. The results might be
                    spread across several files in this folder due to
                    the fact that Hadoop might utilize several
                    reducers in parallel. Merge and copy the result
                    set into a single file on your local (gateway
                    node) file system using the following
                    command.</para>
                <programlisting>$ hadoop fs -getmerge OUTPUT1 result.txt </programlisting>
                <para>When this command succeeds successfully, the
                    results will be stored in the <emphasis
                        role="bold">result.txt</emphasis> file on the
                    gateway node's local file system.</para>
                <para>If you are familiar with Python, try to modify
                    the above application to ignore words that contain
                    special characters. Also, look at <xref
                        linkend="MLB_Game_Logs"/> to learn how to use
                    the Pig tool that ships with Hadoop.</para>
            </section>
        </section>
        <section xml:id="MLB_Game_Logs">
            <title>Analyzing MLB Game Logs Using Pig</title>
            <para>Writing MapReduce jobs for Hadoop using Java or
                Python can grow cumbersome. Luckily, Hadoop includes
                two query engines called Pig and Hive, which provide a
                higher level of abstraction for you to work with. This
                secation shows how you can process some Major League
                Baseball (MLB) data on Hadoop using Pig. </para>
            <para>Prior to working through this article, be sure to
                read <xref linkend="setting_Up"/> and <xref
                    linkend="create_Manage_Clusters"/> to set up a
                3-node cluster. Also, take a look at <xref
                    linkend="run_WordCount"/> in order to learn how to
                copy data into Hadoop and run the packaged (Java)
                WordCount application.</para>
            <section xml:id="code_Pig_App">
                <title>Coding the Pig Application</title>
                <para>This application computes the average attendance
                    for every MLB ballpark for the year 2011. To
                    accomplish this, you need MLB game logs for 2011,
                    which you can download from
                    <link xlink:href=" http://www.retrosheet.org/gamelogs/index.html"
                        > http://www.retrosheet.org/gamelogs/index.html</link>. </para>
                <para>The game log information is processed using the
                    following Pig script:</para>
                <programlisting>__ See http://www.retrosheet.org/gamelogs/glfields.txt

raw = load 'input' using PigStorage(',') as (date:bytearray, 
numgames:bytearray,
dayofweek:bytearray,
vteam:bytearray,
vleague:bytearray,
vteamno:bytearray,
hteam:bytearray,
hleage:bytearray,
hteamno:bytearray,
vscore:int,
hscore:int,
outs:int,
dayindicator:bytearray,
completion:bytearray,
forfeit:bytearray,
protest:bytearray,
parkid:bytearray,
attendance:int
);

__ Strip quotes from team using tokenize
final = foreach raw generate FLATTEN(TOKENIZE(hteam)) as team:chararray, attendance;
final2 = foreach (group final by team) generate group, AVG(final.attendance);

store final2 into 'result2';</programlisting>
                <para>SSH into the gateway node. Then cut and paste
                    the above snippet into a text file called
                        <emphasis role="bold"
                        >attendance.pig</emphasis>.</para>
            </section>
            <section xml:id="run_Pig_App">
                <title>Running the Pig Application</title>
                <para>Download the 2011 MLB game logs (a single file)
                    on to your development machine or laptop. Launch a
                    command prompt and run the following commands from
                    to create a folder on the Hadoop file system
                    (called <emphasis role="bold">input</emphasis>)
                    and upload the log data to it.</para>
                <programlisting>$ ssh joe@166.78.132.227 sudo -u hdfs mkdir input</programlisting>
                <programlisting>$ scp -P 9022 GL2011.txt joe@166.78.132.227:input</programlisting>
                <para>To launch the Pig script, SSH into the gateway
                    node and run the following command:</para>
                <programlisting>$ pig attendance.pig</programlisting>
                <para>When the script completes, run the following
                    command to merge and pull the resulting output
                    from HDFS onto the gateway node's local
                    disk:</para>
                <programlisting>$ hadoop fs -getmerge result RESULT.TXT</programlisting>
                <para>The <emphasis role="bold">RESULT.TXT</emphasis>
                    file should contain the average attendance for
                    every ballpark for 2011. You can view the results
                    using the following command:</para>
                <programlisting>$ less RESULT.TXT</programlisting>
                <para>Pig is a very powerful querying tool and can
                    accomplish many tasks. Take a look at <link
                        xlink:href="http://pig.apache.org/docs/r0.7.0/piglatin_ref1.html"
                        >Pig Latin Reference Manual</link> to learn
                    more about Pig's capabilities.</para>
            </section>

        </section>
-->
    </chapter>
    <chapter xml:id="Additional_Resources">
        <title>Additional Resources</title>
        <para>If you have any questions or concerns about using any of
            the steps in this guide, send an email to
            cbdteam@rackspace.com.</para>
        <para>For information about all Cloud Big Data API operations,
            see the <citetitle>Cloud Big Data Developer
                Guide</citetitle> at <link
                xlink:href="http://docs.rackspace.com/"
                >http://docs.rackspace.com/</link>. All you
            need to get started with Cloud Big Data is this getting
            started guide and the developer guide. </para>
        <para>For more details about Rackspace &PRODNAME;, go to <link
            xlink:href="http://www.rackspace.com/cloud/big-data/"
            >http://www.rackspace.com/cloud/big-data/</link>.
            This site also offers links to official Rackspace support
            channels, including knowledge center articles, forums,
            phone, chat, and email. </para>
        <para>Visit the <link
                xlink:href="http://feedback.rackspacecloud.com/forums/71021-product-feedback/"
                >Product Feedback Forum</link> and tell us what you
            think about &PRODNAME;.</para>
        <para>This API uses standard HTTP 1.1 response codes as
            documented at <link
                xlink:href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"
                >www.w3.org/Protocols/rfc2616/rfc2616-sec10.html</link>. </para>
    </chapter>
</book>
